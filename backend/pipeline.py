"""
This script defines a pipeline for evaluating the performance of different Large Language Models (LLMs)
on a question-answering task. The pipeline crawls specified websites for content, generates answers to a 
set of questions using various LLM providers, and logs the experiment results for analysis.

Key Components:
- Configuration: Sets up paths for prompts and outputs, and lists the websites to be crawled.
- Logging: `log_experiment` function to record the details of each experiment run.
- PromptManager: A class to load and manage the prompts used for the LLMs.
- WebContentCrawler: A class to crawl and extract content from websites asynchronously.
- Question Loader: `load_mock_questions` function to load questions from text files specific to each website.
- Main Execution: `run_full_evaluation` function that orchestrates the entire process, from crawling to 
  generating responses and logging the results.

The script is designed to be run from the command line, which triggers the `run_full_evaluation` function.
"""
import json
from datetime import datetime
from pathlib import Path
import openai
import asyncio
from crawl4ai import AsyncWebCrawler
from helper_classes.model_controller import ModelController, ModelProvider

# Configuration
PROMPT_PATH = "../src/prompts/summary_prompt.txt"
OUTPUT_PATH = "../src/llm_outputs"
WEBSITE_LIST = [
    "http://localhost:3000/wiki",
    "http://localhost:3000/media",
    "http://localhost:3000/jsheavy",
    "http://localhost:3000/interactiveEasy",
    "http://localhost:3000/interactiveHard",
    "http://localhost:3000/semantic"
]

# Logging
def log_experiment(website: str, query: str, model: str, output: str, filename: str = "backend/experiments.jsonl"):
    """
    Logs the details of a single experiment to a JSONL file.

    Args:
        website (str): The URL of the website that was crawled.
        query (str): The question that was asked to the model.
        model (str): The name of the language model used.
        output (str): The output generated by the model.
        filename (str, optional): The path to the log file. Defaults to "backend/experiments.jsonl".
    """
    entry = {
        "timestamp": datetime.utcnow().isoformat(),
        "website": website,
        "query": query,
        "model": model,
        "output": output
    }
    with open(filename, "a", encoding="utf-8") as f:
        f.write(json.dumps(entry) + "\n")

# Prompt Manager
class PromptManager:
    """Manages loading of prompts from text files."""
    def __init__(self, prompt_path):
        """
        Initializes the PromptManager with the path to the prompt file.

        Args:
            prompt_path (str): The path to the prompt file.
        """
        self.prompt_path = Path(prompt_path)
    
    def load_prompt(self):
        """
        Loads the prompt from the file specified in the constructor.

        Returns:
            str: The content of the prompt file, or an empty string if the file is not found.
        """
        try:
            with open(self.prompt_path, 'r', encoding='utf-8') as f:
                return f.read().strip()
        except FileNotFoundError:
            print(f"Warning: Prompt file not found at {self.prompt_path}")
            return ""

# Crawler
class WebContentCrawler:
    """Crawls web content asynchronously and returns it in Markdown format."""
    async def crawl_content(self, url):
        """
        Asynchronously crawls the given URL and extracts its content.

        Args:
            url (str): The URL of the website to crawl.

        Returns:
            dict: A dictionary containing the crawling result. 
                  - 'success' (bool): True if crawling was successful, False otherwise.
                  - 'content' (str): The crawled content in Markdown format (if successful).
                  - 'error' (str): The error message (if crawling failed).
                  - 'url' (str): The URL that was crawled.
                  - 'timestamp' (str): The ISO format timestamp of when the crawling was done.
        """
        print(url)
        async with AsyncWebCrawler(verbose=True) as crawler:
            try:
                result = await crawler.arun(url=url)
                if result.success:
                    return {
                        'success': True,
                        'content': result.markdown,
                        'url': url,
                        'timestamp': datetime.now().isoformat()
                    }
                else:
                    return {
                        'success': False,
                        'error': 'Crawling failed',
                        'url': url,
                        'timestamp': datetime.now().isoformat()
                    }
            except Exception as e:
                return {
                    'success': False,
                    'error': str(e),
                    'url': url,
                    'timestamp': datetime.now().isoformat()
                }

# Question Loader
def load_mock_questions(website: str, base_path="backend/questions"):
    """
    Loads mock questions for a given website from a text file.

    The function expects a file named after the website's domain in the `base_path` directory.
    For example, for 'https://google.com', it will look for 'google_com.txt'.

    Args:
        website (str): The URL of the website.
        base_path (str, optional): The base directory where question files are stored. 
                                Defaults to "backend/questions".

    Returns:
        list: A list of questions as strings.
    """
    domain_key = website.replace("https://", "").replace(".", "_")
    question_file = Path(base_path) / f"{domain_key}.txt"
    with open(question_file, "r", encoding="utf-8") as f:
        return [line.strip() for line in f if line.strip()]

# Main execution
async def run_full_evaluation():
    """
    Runs the full evaluation pipeline.

    This function orchestrates the process of:
    1. Loading the prompt.
    2. Crawling each website in WEBSITE_LIST.
    3. Loading mock questions for each website.
    4. Generating answers using different LLM providers.
    5. Logging the results of each experiment.
    6. Printing a summary of the evaluation at the end.
    """
    prompt_manager = PromptManager(PROMPT_PATH)
    prompt_template = prompt_manager.load_prompt()
    crawler = WebContentCrawler()
    all_results = []

    for website in WEBSITE_LIST:
        crawl_result = await crawler.crawl_content(website)
        if not crawl_result['success']:
            print(f"Crawling failed: {crawl_result['error']}")
            continue
        content = crawl_result['content']
        questions = load_mock_questions(website)

        for provider in ModelProvider:
            controller = ModelController(provider)
            for question in questions:
                full_prompt = f"{prompt_template}\n\nWebsite Content:\n{content}\n\nQuestion: {question}"
                try:
                    response_text = controller.generate_text(full_prompt)
                    result_data = {
                        "success": True,
                        "content": response_text,
                        "model": provider.value,
                        "question": question,
                        "timestamp": datetime.now().isoformat()
                    }
                except Exception as e:
                    print(e)
                    result_data = {
                        "success": False,
                        "error": str(e),
                        "model": provider.value,
                        "question": question,
                        "timestamp": datetime.now().isoformat()
                    }

                log_experiment(website, question, provider.value, result_data.get("content", "ERROR"))
                all_results.append((website, provider.value, question, result_data['success']))

    print("\n--- Evaluation complete ---")
    for w, m, q, s in all_results:
        print(f"{w} | {m} | {q[:30]}... | {'yes' if s else 'no'}")

if __name__ == "__main__":
    # run the evaluation
    asyncio.run(run_full_evaluation())